# Training configuration for LLaVA-1.5-7B with LRV-Instruct dataset
# Using LoRA + DeepSpeed ZeRO-2 for distributed training

run:
  task: DatasetTrainTaskWithCustomModel
  runner: CustomSFTTrainer
  seed: 2025

model:
  model_cls: LlavaForConditionalGeneration
  config_cls: LlavaConfig
  model_cls_config:
    pretrained_model_name_or_path: llava-hf/llava-1.5-7b-hf
    dtype: float16
  config_cls_config:
    architectures:
      - LlavaForConditionalGeneration
    ignore_index: -100
    image_token_index: 32000
    model_type: llava
    pad_token_id: 32001
    projector_hidden_act: gelu
    text_config:
      model_type: llama_real
      hidden_size: 4096
      vocab_size: 32064
      text_config:
        _name_or_path: lmsys/vicuna-7b-v1.5
        architectures:
          - LlamaForCausalLM
        max_position_embeddings: 4096
        model_type: llama
        rms_norm_eps: 1.0e-05
        torch_dtype: float16
        vocab_size: 32064
      additional_attention_module_config:
        model_type: reweight_attention
        num_attention_heads: 32
        num_key_value_heads: 32
        head_dim: 128
        rank_dim: 16
    tie_word_embeddings: false
    vision_config:
      hidden_size: 1024
      image_size: 336
      intermediate_size: 4096
      model_type: clip_vision_model
      num_attention_heads: 16
      num_hidden_layers: 24
      patch_size: 14
      projection_dim: 768
      vocab_size: 32000

processor:
  config:
    pretrained_model_name_or_path: llava-hf/llava-1.5-7b-hf
    use_fast: true

dataset:
  LRVInstructDatasetBuilder:
    split: train

collator:
  collator_cls: LRVInstructForSFTImageCollator
  config: {}

trainer:
  output_dir: ./checkpoints/llava-real-lrv
  num_train_epochs: 3
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 1
  learning_rate: 2.0e-5
  weight_decay: 0.0
  warmup_ratio: 0.03
  lr_scheduler_type: cosine
  logging_steps: 1
  save_strategy: epoch
  save_total_limit: 2
  fp16: true
  bf16: false
  dataloader_num_workers: 4
  remove_unused_columns: false
  report_to: wandb
  run_name: llava-lrv-lora-deepspeed
  # DeepSpeed ZeRO-2 configuration
#  deepspeed: config/deepspeed/ds_config_zero2.json
